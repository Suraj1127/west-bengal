{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c51eb3b7-4ebb-487c-a8f8-aaaa6c5d9685",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark notebook for initial processing of raw mobility data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs a spatial join of administrative codes onto mobility data points falling within administrative bounds. Spatial joins are famously computationally expensive operations and we do this at a very high level of detail for massive amounts of points. Thus, this is an expensive to run notebook. The benefit is having paid that expense once, you save yourself money going forward: all your data is henceforth highly spatially accurate and analysis can be performed just by looking at the admin codes, without actually manipulating spatial objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ea5018d-2ddc-4040-b4bb-014ef1e075bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">PyPI package pandas has been installed already. The previously-installed package is `pandas==1.0.4`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package numpy has been installed already. The previously-installed package is `numpy==1.19`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package geopandas has been installed already. The previously-installed package is `geopandas`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package descartes has been installed already. The previously-installed package is `descartes`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package fiona has been installed already. The previously-installed package is `fiona`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package shapely has been installed already. The previously-installed package is `shapely`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package pyproj has been installed already. The previously-installed package is `pyproj`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package matplotlib has been installed already. The previously-installed package is `matplotlib`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package pyspark has been installed already. The previously-installed package is `pyspark`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "PyPI package geospark has been installed already. The previously-installed package is `geospark`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\n",
       "Out[8]: True</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">PyPI package pandas has been installed already. The previously-installed package is `pandas==1.0.4`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package numpy has been installed already. The previously-installed package is `numpy==1.19`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package geopandas has been installed already. The previously-installed package is `geopandas`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package descartes has been installed already. The previously-installed package is `descartes`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package fiona has been installed already. The previously-installed package is `fiona`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package shapely has been installed already. The previously-installed package is `shapely`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package pyproj has been installed already. The previously-installed package is `pyproj`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package matplotlib has been installed already. The previously-installed package is `matplotlib`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package pyspark has been installed already. The previously-installed package is `pyspark`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package geospark has been installed already. The previously-installed package is `geospark`. To resolve this issue detach and re-attach the notebook to create a new environment or rename the package.\nOut[8]: True</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# installing the python packages using dbutils\n",
    "dbutils.library.installPyPI(\"pandas\",'1.0.4')\n",
    "dbutils.library.installPyPI(\"numpy\",'1.19')\n",
    "dbutils.library.installPyPI(\"geopandas\")\n",
    "dbutils.library.installPyPI(\"descartes\")\n",
    "dbutils.library.installPyPI(\"fiona\")\n",
    "dbutils.library.installPyPI(\"shapely\")\n",
    "dbutils.library.installPyPI(\"pyproj\")\n",
    "dbutils.library.installPyPI(\"matplotlib\")\n",
    "dbutils.library.installPyPI(\"pyspark\")\n",
    "dbutils.library.installPyPI(\"geospark\")\n",
    "\n",
    "# importing the required python packages\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from geospark.register import upload_jars\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "\n",
    "upload_jars() # necessary to load in GeoSpark libraries manually installed to the server\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b585e3f-bd7f-4903-a576-9a1e038cdb59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import a more extensive list of pySpark and GeoSpark packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fs\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import pylab as plt\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import acos, cos, sin, lit, toRadians\n",
    "\n",
    "import geospark\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "from geospark.utils import GeoSparkKryoRegistrator, KryoSerializer\n",
    "from geospark.register import upload_jars\n",
    "from geospark.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from geospark.core import SpatialRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14dafcc8-74be-4303-ac9a-651225b46e64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# These help optimize the operations of Spark\n",
    "# Reference for partitions: https://stackoverflow.com/questions/35800795/number-of-partitions-in-rdd-and-performance-in-spark and https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\n",
    "# Reference for memory / ResultSize limits: https://stackoverflow.com/questions/47996396/total-size-of-serialized-results-of-16-tasks-1048-5-mb-is-bigger-than-spark-dr\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5000) # above 2000 it uses a different process. Setting to 500 so ( ( 56 / 8) * 120) / 5000) = 168 MB per partition\n",
    "spark.conf.set(\"spark.network.timeout\", 10000)\n",
    "spark.conf.set(\"spark.driver.memory\", 0)\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", '16g')\n",
    "spark.conf.set(\"spark.serializer\", KryoSerializer.getName)\n",
    "# spark/conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName)\n",
    "spark.conf.set(\"geospark.global.index\",\"true\") # test\n",
    "spark.conf.set(\"geospark.join.gridtype\",\"kdbtree\")\n",
    " \n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa62789b-b211-4e52-9189-7164841a6172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import com.vividsolutions.jts.geom.{Coordinate, Geometry, GeometryFactory}\n",
       "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "import org.datasyslab.geospark.spatialRDD.SpatialRDD\n",
       "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "import org.datasyslab.geospark.spatialPartitioning\n",
       "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "import org.datasyslab.geosparksql.utils.{Adapter, GeoSparkSQLRegistrator}\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import com.vividsolutions.jts.geom.{Coordinate, Geometry, GeometryFactory}\nimport org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\nimport org.datasyslab.geospark.spatialRDD.SpatialRDD\nimport org.datasyslab.geospark.enums.{GridType, IndexType}\nimport org.datasyslab.geospark.spatialPartitioning\nimport org.datasyslab.geospark.spatialOperator.JoinQuery\nimport org.datasyslab.geosparksql.utils.{Adapter, GeoSparkSQLRegistrator}\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// import various GeoSpark libraries. \n",
    "// Full set of libraries can be tediously found here: https://github.com/DataSystemsLab/GeoSpark/tree/master/core/src/main/java/org/datasyslab/geospark\n",
    "\n",
    "import com.vividsolutions.jts.geom.{Coordinate, Geometry, GeometryFactory}\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import org.datasyslab.geospark.spatialRDD.SpatialRDD\n",
    "import org.datasyslab.geospark.enums.{GridType,IndexType}\n",
    "import org.datasyslab.geospark.spatialPartitioning\n",
    "import org.datasyslab.geospark.spatialOperator.{JoinQuery}\n",
    "import org.datasyslab.geosparksql.utils.{Adapter, GeoSparkSQLRegistrator}\n",
    "GeoSparkSQLRegistrator.registerAll(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "393b7d9b-0e52-4ab4-b25d-0cd98bab787b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### One time -- load in administrative unit shapefiles being used to join codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aee20987-8b6e-4373-8e12-939a623318ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Only need to do this one time and save out the geopackage to a global table\n",
    "\n",
    "# # read admin shapefiles from the GeoPackage\n",
    "# admin1 = gp.read_file(\"/dbfs/mnt/CUBEIQ/esapv/India/India_Administrative_Boundaries.gpkg\",layer=\"Admin1\")\n",
    "# admin2 = gp.read_file(\"/dbfs/mnt/CUBEIQ/esapv/India/India_Administrative_Boundaries.gpkg\",layer=\"Admin2\")\n",
    "# # admin3 = gp.read_file(\"/dbfs/mnt/CUBEIQ/esapv/India/India_Administrative_Boundaries.gpkg\",layer=\"Admin3\")\n",
    "# admin5 = gp.read_file(\"/dbfs/mnt/CUBEIQ/esapv/India/India_Administrative_Boundaries.gpkg\",layer=\"Admin5_TownVillageWard_FullCodes\")\n",
    "\n",
    "# # Enable Arrow-based columnar data transfer for speeding up this process\n",
    "# # Interestingly this operation can NOT make use of additional workers, which is why it takes over an hour to convert Admin3 and admin5 to dataframes. Geopandas is not at all optimized for distributed computing, basically.\n",
    "\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# # Creating spark dataframes from the geopandas GeoDataFrames\n",
    "\n",
    "# admin1 = spark.createDataFrame(\n",
    "#     admin1[['L1_CODE', 'geometry']]\n",
    "#   )\n",
    "\n",
    "# admin2 = spark.createDataFrame(\n",
    "#     admin2[['L1_CODE', 'L2_CODE', 'geometry']]\n",
    "#   )\n",
    "\n",
    "# admin5 = spark.createDataFrame(\n",
    "#     admin5[['L1_CODE', 'L2_CODE', 'L3_CODE','L4_CODE', 'geo_id', 'geometry']]\n",
    "#   )\n",
    "\n",
    "# admin1.write.saveAsTable(\"IN_adm1\")\n",
    "# admin2.write.saveAsTable(\"IN_adm2\")\n",
    "# admin5.write.saveAsTable(\"IN_adm5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec4349b9-3f0c-40a9-80b3-337bf5e49bc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "52dd3693-c0d3-4da8-bc18-d7355e0509cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "sdf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "device_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "device_os",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "timestamp",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "lat",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "lon",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "accuracy",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "country",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        },
        {
         "name": "points",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "device_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "timestamp",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "geometry",
            "nullable": false,
            "type": {
             "class": "org.apache.spark.sql.geosparksql.UDT.GeometryUDT",
             "pyClass": "geospark.sql.types.GeometryType",
             "sqlType": {
              "containsNull": false,
              "elementType": "byte",
              "type": "array"
             },
             "type": "udt"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the whole vendor's data into a spark dataframe -- throughout this notebook we are using Veraset as an example\n",
    "sdf = sqlContext.sql(\"SELECT * FROM veraset_1 WHERE veraset_1.country = 'IN'\")\n",
    "\n",
    "# register sdf spark dataframe as sdf_ tempview\n",
    "sdf.createOrReplaceTempView('sdf_')\n",
    "\n",
    "# delta out path variable\n",
    "\n",
    "out_delta_path = \"/mnt/CUBEIQ/esapv/India/delta_veraset\"\n",
    "filter_range = -60\n",
    "\n",
    "# converting the long and lat columns into Point(X, Y) column using geospark\n",
    "\n",
    "points = spark.sql(f\"SELECT device_id, \\\n",
    "                           timestamp, \\\n",
    "                           to_date(from_unixtime(timestamp,'yyyy-MM-dd')) as date, \\\n",
    "                           ST_Point(CAST(lon AS Decimal(24,20)),CAST(lat AS Decimal(24,20))) AS geometry \\\n",
    "                   FROM Veraset_1 \\\n",
    "                   WHERE Veraset_1.country = 'IN' AND timestamp > unix_timestamp(date_add(current_date(), {filter_range})) \") # loads within current date - filter range\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df4c2fc7-6580-4089-9da4-b19dad2d8da5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "admin5",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "L1_CODE",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "L2_CODE",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "L3_CODE",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "L4_CODE",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "geo_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "geometry",
            "nullable": true,
            "type": {
             "class": "org.apache.spark.sql.geosparksql.UDT.GeometryUDT",
             "pyClass": "geospark.sql.types.GeometryType",
             "sqlType": {
              "containsNull": false,
              "elementType": "byte",
              "type": "array"
             },
             "type": "udt"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the geospatial administrative files directly from a global table\n",
    "admin5 = sqlContext.sql(\"SELECT * FROM IN_adm5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f4d4aa4-9646-4307-9358-183d3d15ed6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now register spark dataframes as tables\n",
    "sqlContext.registerDataFrameAsTable(points, \"points\")\n",
    "sqlContext.registerDataFrameAsTable(admin5, \"admin5_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "414bde02-0295-4c91-b17e-de2e4b8e51ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "points1",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "device_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "timestamp",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "geometry",
            "nullable": false,
            "type": {
             "class": "org.apache.spark.sql.geosparksql.UDT.GeometryUDT",
             "pyClass": "geospark.sql.types.GeometryType",
             "sqlType": {
              "containsNull": false,
              "elementType": "byte",
              "type": "array"
             },
             "type": "udt"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here data can be filtered using some date range; this is optional. Cuts down on processing time and keeps it from crashing for truly massive (India scale) tasks\n",
    "from_ = \"01/01/2021\"\n",
    "to_ = \"15/02/2021\"\n",
    "\n",
    "from_strp = time.mktime(datetime.datetime.strptime(from_, \"%d/%m/%Y\").timetuple())\n",
    "to_strp = time.mktime(datetime.datetime.strptime(to_, \"%d/%m/%Y\").timetuple())\n",
    "\n",
    "timestamps = (from_strp, to_strp)\n",
    "points1 = points.where(col('timestamp').between(*timestamps))\n",
    "\n",
    "# now register this as a table\n",
    "sqlContext.registerDataFrameAsTable(points1, \"pts_filter_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7efa1865-7055-4b79-a72b-f56e15838fc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spatial join and export to delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1577772-6522-49cb-a1e6-a7f9d19ebe00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "spatial_join_result_final",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "adm1_code",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "adm2_code",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "adm3_code",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "adm4_code",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "adm5_code",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "device_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "timestamp",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "geometry",
            "nullable": false,
            "type": {
             "class": "org.apache.spark.sql.geosparksql.UDT.GeometryUDT",
             "pyClass": "geospark.sql.types.GeometryType",
             "sqlType": {
              "containsNull": false,
              "elementType": "byte",
              "type": "array"
             },
             "type": "udt"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spatial join for adm5\n",
    "# This is a bit brute force and computationally expensive. A more refined approach would use H3. One for the future.\n",
    "\n",
    "intersect_query_adm5 = \"\"\"\n",
    "  SELECT \n",
    "        s.L1_CODE as adm1_code, \n",
    "        s.L2_CODE as adm2_code, \n",
    "        s.L3_CODE as adm3_code, \n",
    "        s.L4_CODE as adm4_code,\n",
    "        s.geo_id as adm5_code, \n",
    "        p.device_id, \n",
    "        p.timestamp, \n",
    "        p.date,\n",
    "        p.geometry \n",
    "  FROM pts_filter_tbl AS p, admin5_tbl AS s \n",
    "  WHERE ST_Intersects(p.geometry, s.geometry)\n",
    "\"\"\"\n",
    "\n",
    "spatial_join_result_final = spark.sql(intersect_query_adm5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a57b2bb8-3af2-4522-abf0-28eb140ee149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1733.save.\n",
       ": org.apache.spark.sql.AnalysisException: cannot resolve &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; due to data type mismatch: differing types in &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; (date and double).; line 1 pos 0;\n",
       "&#39;Filter ((cast(partitionValues#814[date] as date) &gt;= ((cast(1 as double) / cast(1 as double)) / cast(2021 as double))) &amp;&amp; (cast(partitionValues#814[date] as date) &lt;= ((cast(15 as double) / cast(2 as double)) / cast(2021 as double))))\n",
       "+- LocalRelation [path#813, partitionValues#814, size#815L, modificationTime#816L, dataChange#817, stats#818, tags#819]\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:205)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n",
       "\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:200)\n",
       "\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:206)\n",
       "\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:67)\n",
       "\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3548)\n",
       "\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1527)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.filterFileList(DeltaLog.scala:883)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:117)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:71)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:70)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:448)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:70)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:69)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:828)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)\n",
       "\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:128)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n",
       "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n",
       "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:232)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:184)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-482151785836143&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>                                 <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;replaceWhere&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">f&#34;date &gt;= {from_} AND date &lt;= {to_}&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>                                 <span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;date&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">                                 </span><span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>out_delta_path<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n",
       "<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; due to data type mismatch: differing types in &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; (date and double).; line 1 pos 0;\\n&#39;Filter ((cast(partitionValues#814[date] as date) &gt;= ((cast(1 as double) / cast(1 as double)) / cast(2021 as double))) &amp;&amp; (cast(partitionValues#814[date] as date) &lt;= ((cast(15 as double) / cast(2 as double)) / cast(2021 as double))))\\n+- LocalRelation [path#813, partitionValues#814, size#815L, modificationTime#816L, dataChange#817, stats#818, tags#819]\\n&#34;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1733.save.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; due to data type mismatch: differing types in &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; (date and double).; line 1 pos 0;\n&#39;Filter ((cast(partitionValues#814[date] as date) &gt;= ((cast(1 as double) / cast(1 as double)) / cast(2021 as double))) &amp;&amp; (cast(partitionValues#814[date] as date) &lt;= ((cast(15 as double) / cast(2 as double)) / cast(2021 as double))))\n+- LocalRelation [path#813, partitionValues#814, size#815L, modificationTime#816L, dataChange#817, stats#818, tags#819]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:200)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:206)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:67)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3548)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1527)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.filterFileList(DeltaLog.scala:883)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:117)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:71)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1$$anonfun$apply$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:448)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:70)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:828)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:128)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:184)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-482151785836143&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>                                 <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;replaceWhere&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">f&#34;date &gt;= {from_} AND date &lt;= {to_}&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>                                 <span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;date&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">                                 </span><span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>out_delta_path<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; due to data type mismatch: differing types in &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; (date and double).; line 1 pos 0;\\n&#39;Filter ((cast(partitionValues#814[date] as date) &gt;= ((cast(1 as double) / cast(1 as double)) / cast(2021 as double))) &amp;&amp; (cast(partitionValues#814[date] as date) &lt;= ((cast(15 as double) / cast(2 as double)) / cast(2021 as double))))\\n+- LocalRelation [path#813, partitionValues#814, size#815L, modificationTime#816L, dataChange#817, stats#818, tags#819]\\n&#34;</div>",
       "errorSummary": "org.apache.spark.sql.AnalysisException: cannot resolve &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; due to data type mismatch: differing types in &#39;(CAST(`partitionValues`[&#39;date&#39;] AS DATE) &gt;= ((CAST(1 AS DOUBLE) / CAST(1 AS DOUBLE)) / CAST(2021 AS DOUBLE)))&#39; (date and double).; line 1 pos 0;",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exporting spatial join results as a delta table we can access at any time\n",
    "# Partitioning by date because most queries are by date and file sizes are roughly the same, reducing skewness across dates when accessing the data. Less skewness = less processing time = less money spent.\n",
    "# Based off here https://docs.databricks.com/delta/intro-notebooks.html\n",
    "\n",
    "spatial_join_result_final.write.format(\"delta\")\\\n",
    "                                .mode(\"overwrite\")\\\n",
    "                                .option(\"replaceWhere\", f\"date >= {from_} AND date <= {to_}\") \\\n",
    "                                .partitionBy(\"date\")\\\n",
    "                                .save(out_delta_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Veraset India 01 Spatial Join",
   "notebookOrigID": 482151785836126,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
