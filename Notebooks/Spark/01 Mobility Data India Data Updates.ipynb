{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f77cc0a-adc3-4154-992a-bbbf0e91a029",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Updating processed mobility data with additional records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb99cb8e-012d-44f6-87b3-160df517149a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First load in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ea5018d-2ddc-4040-b4bb-014ef1e075bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# installing the python packages using dbutils\n",
    "dbutils.library.installPyPI(\"numpy\",'1.19')\n",
    "dbutils.library.installPyPI(\"descartes\")\n",
    "dbutils.library.installPyPI(\"fiona\")\n",
    "dbutils.library.installPyPI(\"shapely\")\n",
    "dbutils.library.installPyPI(\"pyproj\")\n",
    "dbutils.library.installPyPI(\"matplotlib\")\n",
    "dbutils.library.installPyPI(\"pyspark\")\n",
    "dbutils.library.installPyPI(\"geospark\")\n",
    "\n",
    "# importing the required python packages\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "from geospark.register import upload_jars\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "\n",
    "upload_jars() # necessary to load in GeoSpark libraries manually installed to the server\n",
    "\n",
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b585e3f-bd7f-4903-a576-9a1e038cdb59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import a more extensive list of pySpark and GeoSpark packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# General pyspark SQL stuff\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fs\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# date time functions needed for filtering\n",
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime, to_date, to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "# delta tables necessary for pyspark manipulation of deltas\n",
    "from delta.tables import *\n",
    "\n",
    "import pylab as plt\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import acos, cos, sin, lit, toRadians\n",
    "\n",
    "# Geospark-py and SQL stuff\n",
    "import geospark\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "from geospark.utils import GeoSparkKryoRegistrator, KryoSerializer\n",
    "from geospark.register import upload_jars\n",
    "from geospark.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from geospark.core import SpatialRDD\n",
    "from geospark.sql.types import GeometryType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14dafcc8-74be-4303-ac9a-651225b46e64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# these help optimize the operations of Spark\n",
    "# Reference for partitions: https://stackoverflow.com/questions/35800795/number-of-partitions-in-rdd-and-performance-in-spark and https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\n",
    "# Reference for memory / ResultSize limits: https://stackoverflow.com/questions/47996396/total-size-of-serialized-results-of-16-tasks-1048-5-mb-is-bigger-than-spark-dr\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5000) # above 2000 it uses a different process. Setting to 500 so ( ( 56 / 8) * 120) / 5000) = 168 MB per partition\n",
    "spark.conf.set(\"spark.network.timeout\", 10000)\n",
    "spark.conf.set(\"spark.driver.memory\", 0)\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", '16g')\n",
    "spark.conf.set(\"spark.serializer\", KryoSerializer.getName)\n",
    "# spark/conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName)\n",
    "spark.conf.set(\"geospark.global.index\",\"true\") # test\n",
    "spark.conf.set(\"geospark.join.gridtype\",\"kdbtree\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa62789b-b211-4e52-9189-7164841a6172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// import various GeoSpark libraries. \n",
    "// Full set of libraries can be tediously found here: https://github.com/DataSystemsLab/GeoSpark/tree/master/core/src/main/java/org/datasyslab/geospark\n",
    "\n",
    "import com.vividsolutions.jts.geom.{Coordinate, Geometry, GeometryFactory}\n",
    "import com.vividsolutions.jts.geom.Geometry\n",
    "\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import org.datasyslab.geospark.spatialRDD.SpatialRDD\n",
    "import org.datasyslab.geospark.enums.{GridType,IndexType}\n",
    "import org.datasyslab.geospark.spatialPartitioning\n",
    "import org.datasyslab.geospark.spatialOperator.{JoinQuery}\n",
    "import org.datasyslab.geosparksql.utils.{Adapter, GeoSparkSQLRegistrator}\n",
    "import org.datasyslab.geosparksql.UDT._\n",
    "\n",
    "import org.apache.spark.sql.catalyst.util.{ArrayData, GenericArrayData}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.{rand, to_date, from_unixtime, unix_timestamp, _}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "GeoSparkSQLRegistrator.registerAll(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44f55df1-dea5-466e-93dd-48c3b00cfd26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Declare variables and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e71233b-454e-4ad8-a360-c17e614378f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "provider = 'Veraset'\n",
    "# provider = 'Unacast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42806d17-23ac-4845-b88c-2b0969711d8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "out_gtable = f'IN_{provider}_SJ'\n",
    "\n",
    "filter_range = -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "327514be-1875-431f-b456-3856d197c877",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if provider == 'Unacast':\n",
    "  out_delta_path = \"/mnt/CUBEIQ/esapv/India/delta_unacast\"\n",
    "else:\n",
    "  out_delta_path = \"/mnt/CUBEIQ/esapv/India/delta_veraset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec4349b9-3f0c-40a9-80b3-337bf5e49bc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load all the provider's data and India admin geospatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8de820c7-8842-497c-b822-0eff9d0489f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting the long and lat columns into Point(X, Y) column using geospark\n",
    "\n",
    "points = spark.sql(f\"SELECT device_id, \\\n",
    "                           timestamp, \\\n",
    "                           to_date(from_unixtime(timestamp,'yyyy-MM-dd')) as date, \\\n",
    "                           ST_Point(CAST(lon AS Decimal(24,20)),CAST(lat AS Decimal(24,20))) AS geometry \\\n",
    "                   FROM {provider}_1 \\\n",
    "                   WHERE {provider}_1.country = 'IN' AND timestamp > unix_timestamp(date_add(current_date(), {filter_range})) \") # loads within current date - filter range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ca329ed-b2dd-4e38-b9b2-0fc742e36252",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the geospatial administrative files directly from a global table\n",
    "admin5 = sql(\"SELECT * FROM IN_adm5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e22f798-b41b-4c3c-857f-4bcb8036875f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now register spark dataframes as tables\n",
    "points.createOrReplaceTempView(\"pts_tbl\")\n",
    "admin5.createOrReplaceTempView(\"admin5_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7efa1865-7055-4b79-a72b-f56e15838fc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define spatial join query, using GeoSpark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this code is lazy, meaning we don't actually call this operation until the deltaTable operations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1577772-6522-49cb-a1e6-a7f9d19ebe00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spatial join for adm5\n",
    "intersect_query_adm5 = \"\"\"\n",
    "  SELECT \n",
    "        s.L1_CODE as adm1_code, \n",
    "        s.L2_CODE as adm2_code, \n",
    "        s.L3_CODE as adm3_code, \n",
    "        s.L4_CODE as adm4_code,\n",
    "        s.geo_id as adm5_code, \n",
    "        p.device_id, \n",
    "        p.timestamp, \n",
    "        p.date,\n",
    "        p.geometry \n",
    "  FROM pts_tbl AS p, admin5_tbl AS s \n",
    "  WHERE ST_Intersects(p.geometry, s.geometry)\n",
    "\"\"\"\n",
    "\n",
    "spatial_join_result_final = spark.sql(intersect_query_adm5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a21c75f3-0244-4b44-bcca-f55a331139b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import existing delta data, run update on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c15f57fe-6c33-47ef-9797-5cad773705d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, out_delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only update where ID *and* timestamp don't match, to ensure new records for existing IDs are populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a57b2bb8-3af2-4522-abf0-28eb140ee149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spatial joining and inserting missing data into the delta table\n",
    "# Only missing records are SJ'd and merged. We find missing records by searching for unique ID + timestamp combinations -- if they aren't matched, we insert.\n",
    "\n",
    "deltaTable.alias(\"dt_master\").merge(\n",
    "    spatial_join_result_final.alias(\"updates\"),\n",
    "    \"dt_master.device_id = updates.device_id AND dt_master.timestamp = updates.timestamp\") \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization keeps your tables nicely streamlined for later analysis. This saves you money + time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca1b4592-c892-42da-88bc-392734247383",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"OPTIMIZE {out_gtable} where date >= (date_add(current_date(),{filter_range})) ZORDER by adm5_code\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Veraset India 02 Updating",
   "notebookOrigID": 2497381830575362,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
