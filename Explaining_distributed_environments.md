
## Explaining Spark and distributed processing environments

Processing mobility datasets at the scale of India, Bangladesh, or other highly populous countries requires computing resources well beyond those of any individual computer. Datasets will stretch into the billions of rows / hundreds of gigabytes and spatial operations on each row can be expensive, even with clever engineering. The solution is to use distributed computing, which chops up analysis tasks into separate pieces, assigns pieces to one of many individual computers, then aggregates the results from all these computers in whatever format you desire.

Distributed environments are very powerful but require a more complete understanding of the computer science fundamentals behind how tasks are handled by computers than running code in STATA, R, or vanilla Python. Engineer your task well and you will reduce the processing time exponentially; engineer it poorly and you will replicate the work many times over or get snagged on critical bottlenecks. Replicating work can impose penalties not only in terms of lost time but lost financial resources if your analysis rents a server of many computers by the computer-hour, as was the case for this analysis.

These notebooks make heavy use of Spark as the foundation for distributed analysis. Spark is an "analytics engine"  which natively manages the distribution of tasks amongst distributed "clusters" of computers. Spark simplifies big data analytics by intelligently dividing up and assigning specified tasks to computers and to the greatest extent possible protecting data scientists from themselves by choosing the least computationally costly methods for doing so. Spark is *not* a language itself, but can be manipulated by Java, Scala, PySpark, or R. Many custom libraries of code are built to execute interesting analysis routines on your data using one of these languages to run Spark itself.

Spark is a separate environment than traditional Python/R and must be approched as such. It must be hosted on a server and its concepts for packaging and distributing data must be understood -- all of which represent non-trivial costs. Users unwilling or unable to take on these added outlays should instead look to Dask, which offers a halfway house of sorts: native Python code can be distributed and no server is needed, although users must still wrap their heads around the mechanics of distributing tasks. Dask can be run on individual computers fairly easily, although larger tasks will need to rent cloud environments offering clusters of computers, such as Coiled. The GFDRR's analysis uses Dask to great effect and it makes more sense for those with well developed Python libraries (or just Python skills) they want to adapt to a distributed environment.
